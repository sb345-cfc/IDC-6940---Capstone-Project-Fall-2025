---
title: "Results"
---

This section presents the performance of all six machine-learning models evaluated on the test dataset. Because the dataset is highly imbalanced (~5% stroke cases), accuracy alone is misleading, so emphasis is placed on **sensitivity**, **specificity**, and **AUC**.

---

# Evaluation Function

All models were evaluated using the same function:

```r
evaluate_model <- function(model, test_data, positive_class = "Yes") {
  pred_class <- predict(model, newdata = test_data)
  pred_prob  <- predict(model, newdata = test_data, type = "prob")[, positive_class]
  
  cm <- confusionMatrix(pred_class, test_data$stroke, positive = positive_class)
  
  roc_obj <- roc(
    response = test_data$stroke,
    predictor = pred_prob,
    levels = c("No", "Yes")
  )
  
  list(
    cm = cm,
    auc = auc(roc_obj),
    roc_obj = roc_obj
  )
}

1. Model Results

Each model was evaluated with the function above:
res_glm   <- evaluate_model(fit_glm, test_data)
res_rpart <- evaluate_model(fit_rpart, test_data)
res_rf    <- evaluate_model(fit_rf, test_data)
res_gbm   <- evaluate_model(fit_gbm, test_data)
res_knn   <- evaluate_model(fit_knn, test_data)
res_svm   <- evaluate_model(fit_svm, test_data)

2. AUC Values

| Model                   | AUC    |
| ----------------------- | ------ |
| Logistic Regression     | 0.8167 |
| Decision Tree           | 0.6950 |
| Random Forest           | 0.8050 |
| Gradient Boosting (GBM) | 0.8100 |
| k-Nearest Neighbors     | 0.6784 |
| SVM (Radial)            | 0.6390 |

Highest AUC: Logistic Regression (0.8167), GBM (0.810), and Random Forest (0.805).

3. Confusion Matrices (Test Set)
Most models predicted every case as “No Stroke”, resulting in 0% sensitivity:

Logistic Regression
res_glm$cm

Decision Tree
res_rpart$cm

Random Forest
res_rf$cm

GBM
res_gbm$cm

kNN
res_knn$cm

SVM
res_svm$cm

Across models:

- TN (True Negatives) were high

- FP (False Positives) were very low

- TP = 0 almost always

- Sensitivity = 0 for 5 out of 6 models

This is a typical outcome in highly imbalanced medical datasets.

4. ROC Curve Comparison
plot(res_glm$roc_obj, col="black", lwd=2, main="ROC Curves for Stroke Prediction (6 Models)")
plot(res_rpart$roc_obj, col="orange", lwd=2, add=TRUE)
plot(res_rf$roc_obj,    col="red",    lwd=2, add=TRUE)
plot(res_gbm$roc_obj,   col="blue",   lwd=2, add=TRUE)
plot(res_knn$roc_obj,   col="brown",  lwd=2, add=TRUE)
plot(res_svm$roc_obj,   col="darkgreen", lwd=2, add=TRUE)

ROC Interpretation

- GBM (blue) and Random Forest (red) show the best separation.

- Logistic Regression (black) also performs well.

- kNN, SVM, and the Decision Tree show weaker performance.

This matches the AUC results.

5. Model Comparison Table
model_comparison <- tibble::tibble(
  Model = c("Logistic Regression", "Decision Tree", "Random Forest",
            "Gradient Boosting (GBM)", "k-Nearest Neighbors", "SVM (Radial)"),
  
  Accuracy = c(res_glm$cm$overall["Accuracy"],
               res_rpart$cm$overall["Accuracy"],
               res_rf$cm$overall["Accuracy"],
               res_gbm$cm$overall["Accuracy"],
               res_knn$cm$overall["Accuracy"],
               res_svm$cm$overall["Accuracy"]),
  
  Sensitivity = c(res_glm$cm$byClass["Sensitivity"],
                  res_rpart$cm$byClass["Sensitivity"],
                  res_rf$cm$byClass["Sensitivity"],
                  res_gbm$cm$byClass["Sensitivity"],
                  res_knn$cm$byClass["Sensitivity"],
                  res_svm$cm$byClass["Sensitivity"]),
  
  Specificity = c(res_glm$cm$byClass["Specificity"],
                  res_rpart$cm$byClass["Specificity"],
                  res_rf$cm$byClass["Specificity"],
                  res_gbm$cm$byClass["Specificity"],
                  res_knn$cm$byClass["Specificity"],
                  res_svm$cm$byClass["Specificity"]),
  
  AUC = c(res_glm$auc, res_rpart$auc, res_rf$auc,
          res_gbm$auc, res_knn$auc, res_svm$auc)
)

model_comparison %>% 
  mutate(across(2:5, round, 4))

Summary of Table

Accuracy is misleading high (~95% for all models)

Sensitivity is nearly zero for most models

GBM, RF, and Logistic show best AUC

Decision Tree performs moderately

kNN and SVM perform poorly

6. Threshold Adjustment (Improving Sensitivity)

Because stroke is rare, using the default probability threshold of 0.5 causes models to miss all positive cases.

We tested a lower threshold of 0.3 for GBM:

probs <- predict(fit_gbm, newdata = test_data, type = "prob")[,"Yes"]
preds <- ifelse(probs > 0.3, "Yes", "No")
confusionMatrix(factor(preds), test_data$stroke, positive="Yes")

Output Summary

- Sensitivity improved from 0% → 8.1%

- Specificity remained high (98.8%)

- Accuracy slightly decreased (95.17% → 94.45%)

- Balanced Accuracy increased (0.50 → 0.53)

- Model correctly identified 6 stroke cases after tuning

Interpretation

Lowering the threshold improves detection of rare events and is a common technique for medical prediction tasks

Final Interpretation of Results

- GBM and Random Forest showed the strongest overall discriminative performance (AUC).

- Logistic Regression surprisingly performed well given its simplicity.

- All models struggled with sensitivity due to the high class imbalance.

- Threshold adjustment improved sensitivity and detection of stroke cases.

- Accuracy alone is misleading for this dataset since predicting “No stroke” yields 95% accuracy.

Summary

This results section demonstrates that:

- GBM is the best-performing model overall

- Sensitivity requires threshold tuning or imbalance techniques

- ROC and AUC give a much clearer picture than accuracy

- Imbalanced medical datasets pose significant modeling challenges

